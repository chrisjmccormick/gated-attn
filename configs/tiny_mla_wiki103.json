{

  "shorthand": "model.256.lyr.6 - seqlen.128 - mla.96.64.0 - ah8.32 - rd16.16",
  "notes": "Tiny MLA baseline configuration trained on wikitext103",
  
  "model": {
  	
	"hidden_size":          256,
    "num_hidden_layers":    6,
    "intermediate_size":    672,
        
<<<<<<< Updated upstream:subspace_decoder/configs/tiny_mla_wiki103.json
    "vocab_size":               50257,
    "tie_word_embeddings":      true,
    "max_position_embeddings":  128,
=======
    "vocab_size":           50257,
    "max_position_embeddings":  1024,
>>>>>>> Stashed changes:subspace_decoder/configs/gpt-2_mla_c4.json

    "norm_type":            "rmsnorm",
    "layer_norm_eps":       1e-12,
    "rms_norm_eps":         1e-6,

	"num_dense_layers":     0,	

    "num_attention_heads":  8,

    "q_shared_dim":         96,	
    "kv_shared_dim":        64,
    "o_shared_dim":         null,    
	
    "qk_private_dim":       32,
	"vo_private_dim":       32,

    "rope_dims":            16,
	"nope_dims":            16,

    "rope_theta":           10000.0,
    "rope_scaling":         null,

	"attention_bias":       false,	
	
    "attention_backend":    "flash_attention_2",

    "ffn_decompose":     false,
    "ffn_rank":          null,
	
	"vocab_subspace":    false,
	"vocab_rank":        null,


    "hidden_dropout_prob":     0.1,
    "attention_dropout_prob":  0.1,
    "classifier_dropout":      null,
    "initializer_range":       0.02
  },
  
  "pre_train": {
    "wandb_project":        "decoder-pretrain-wiki",
    "output_dir":           "checkpoints/tiny_mla_wiki_baseline",
	"seed":                 42,
    "logging_steps":        50,
    "save_steps":           300,

    "train_batch_size":     1024,
    "gradient_accumulation_steps": 1,
    "learning_rate":        5e-04,
    "num_train_steps":      12500,
    "eval_steps":           1000,
    "weight_decay":         0.01,
    "num_workers":          8,
    "pin_memory":           true,

    "dataset_name":         "wikitext",
    "dataset_config":       "wikitext-103-raw-v1",
    "dataset_subset_pct":   null,
    
	"max_seq_length":       1024,
	"eval_batch_size":      64,
	
    "fp16":                 false,
    "bf16":                 true,
    "torch_compile":        true,
    "torch_compile_backend": "inductor",
    "torch_compile_mode":   "default"
  }

}
