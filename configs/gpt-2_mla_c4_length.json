{

  "shorthand": "model.768.lyr.12 - seqlen.128 - mla.192.96.0 - ah12.64 - rd32.32",
  "notes": "MLA Baseline on C4, with Length Scheduling",
  
  "model": {
  	
	"hidden_size":          768,
    "num_hidden_layers":    12,
    "intermediate_size":    2048,
        
    "vocab_size":           50257,
    "max_position_embeddings":  1024,

    "norm_type":            "layernorm",
    "layer_norm_eps":       1e-12,
    "rms_norm_eps":         1e-6,

	"num_dense_layers":     0,	

    "num_attention_heads":  12,

    "q_shared_dim":         192,	
    "kv_shared_dim":        96,
    "o_shared_dim":         null,    
	
    "qk_private_dim":       64,
	"vo_private_dim":       64,

    "rope_dims":            32,
	"nope_dims":            32,

    "rope_theta":           10000.0,
    "rope_scaling":         null,

	"attention_bias":       false,	
	
    "attention_backend":    "sdpa",

    "ffn_decompose":     false,
    "ffn_rank":          null,
	
	"vocab_subspace":    false,
	"vocab_rank":        null,


    "hidden_dropout_prob":     0.1,
    "attention_dropout_prob":  0.1,
    "classifier_dropout":      null,
    "initializer_range":       0.02
  },
  
  "pre_train": {
    "wandb_project":        "decoder-pretrain-c4",
    "output_dir":           "checkpoints/gpt2_mla_c4_length_scheduled",
	"seed":                 42,
		
    "train_batch_size":     32,
    "gradient_accumulation_steps": 32,
    "learning_rate":        5e-04,
    "num_train_steps":      12500,
    "eval_steps":           1000,
    "weight_decay":         0.01,
    "num_workers":          8,
    "pin_memory":           true,

    "dataset_name":         "allenai/c4",
    "dataset_config":       "en",
    "dataset_subset_pct":   0.01,
    
	"max_seq_length":       1024,
	"eval_batch_size":      64,
	
    "fp16":                 false,
    "bf16":                 true,
    "torch_compile":        true,
    "torch_compile_backend": "inductor",
    "torch_compile_mode":   "default",
    
    "length_scheduling": {
      "enabled": true,
      "phases": [
        {
          "name": "short_sequence",
          "seq_length": 512,
          "steps": 10000,
          "description": "80% of training at 512 tokens"
        },
        {
          "name": "medium_sequence", 
          "seq_length": 1024,
          "steps": 2000,
          "description": "16% of training at 1024 tokens"
        },
        {
          "name": "long_sequence",
          "seq_length": 2048,
          "steps": 500,
          "description": "4% of training at 2048 tokens with RoPE scaling",
          "rope_scaling": {
            "type": "linear",
            "factor": 2.0
          }
        }
      ]
    }
  }

}
