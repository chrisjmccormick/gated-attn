{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Benchmarking with LM-Eval-Harness\n",
        "\n",
        "This notebook benchmarks the GPT-2 MLA model defined in `configs/gpt-2_mla_c4.json` using the lm-eval-harness framework. The model uses Multi-Layer Attention (MLA) with shared subspaces for query, key, and value projections.\n",
        "\n",
        "## Model Configuration\n",
        "- **Architecture**: GPT-2 with MLA attention\n",
        "- **Hidden Size**: 768\n",
        "- **Layers**: 12\n",
        "- **Attention Heads**: 12\n",
        "- **Query Shared Dim**: 192\n",
        "- **KV Shared Dim**: 96\n",
        "- **Vocab Size**: 50,257\n",
        "- **Max Position Embeddings**: 1,024\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n",
        "\n",
        "First, we import the necessary libraries and set up logging for the benchmarking process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from lm_eval import simple_evaluate\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Configure the benchmarking parameters. The model path should point to your trained GPT-2 MLA model checkpoint.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Model Configuration ---\n",
        "# IMPORTANT: Update this path to point to your trained model checkpoint\n",
        "# This can be a local path or a model ID on the Hugging Face Hub\n",
        "MODEL_PATH = \"checkpoints/gpt2_mla_c4_baseline\"  # Update this path as needed\n",
        "\n",
        "# If your model requires `trust_remote_code=True`, set this to True\n",
        "# This is common for custom architectures like MLA\n",
        "TRUST_REMOTE_CODE = True\n",
        "\n",
        "# Configure the batch size. \"auto\" lets lm-eval-harness decide the best\n",
        "# batch size. You can also set it to a specific integer like 8 or 16.\n",
        "BATCH_SIZE = \"auto\"\n",
        "\n",
        "# Path to save the detailed results\n",
        "OUTPUT_PATH = \"benchmark_results.json\"\n",
        "\n",
        "print(f\"Model path: {MODEL_PATH}\")\n",
        "print(f\"Trust remote code: {TRUST_REMOTE_CODE}\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task Configuration\n",
        "\n",
        "Define the benchmark tasks to evaluate. We'll test the model on a comprehensive set of tasks covering different capabilities:\n",
        "\n",
        "- **Language Modeling**: WikiText (perplexity)\n",
        "- **Cloze/Prediction**: LAMBADA\n",
        "- **Commonsense Reasoning**: HellaSwag, PIQA, WinoGrande\n",
        "- **Reading Comprehension**: ARC Easy/Challenge, OpenBookQA\n",
        "- **Knowledge**: MMLU (5-shot)\n",
        "- **Mathematical Reasoning**: GSM8K (8-shot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the list of tasks you want to run\n",
        "TASKS = [\n",
        "    \"wikitext\",         # Perplexity on WikiText\n",
        "    \"lambada_openai\",   # Cloze/Prediction task\n",
        "    \"hellaswag\",        # Commonsense NLI\n",
        "    \"piqa\",             # Physical Interaction QA\n",
        "    \"winogrande\",       # Commonsense Reasoning (Winograd Schema)\n",
        "    \"arc_easy\",         # AI2 Reasoning Challenge (Easy)\n",
        "    \"arc_challenge\",    # AI2 Reasoning Challenge (Challenge)\n",
        "    \"openbookqa\",       # Open Book Question Answering\n",
        "    \"mmlu\",             # Massive Multitask Language Understanding\n",
        "    \"gsm8k\",            # Grade School Math\n",
        "]\n",
        "\n",
        "# Define tasks that should be run with a specific number of few-shot examples\n",
        "# The numbers here are standard for these benchmarks\n",
        "FEW_SHOT_CONFIG = {\n",
        "    \"mmlu\": 5,    # 5-shot for MMLU\n",
        "    \"gsm8k\": 8,   # 8-shot for GSM8K\n",
        "}\n",
        "\n",
        "print(f\"Total tasks: {len(TASKS)}\")\n",
        "print(f\"0-shot tasks: {len([t for t in TASKS if t not in FEW_SHOT_CONFIG])}\")\n",
        "print(f\"Few-shot tasks: {len(FEW_SHOT_CONFIG)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Path Validation\n",
        "\n",
        "Check if the model path exists and is valid before proceeding with benchmarking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_model_path(model_path):\n",
        "    \"\"\"\n",
        "    Validate that the model path exists and contains necessary files.\n",
        "    \"\"\"\n",
        "    if model_path == \"path/to/your/custom/decoder/model\":\n",
        "        logger.error(\"=\"*80)\n",
        "        logger.error(\"ERROR: Please update the `MODEL_PATH` variable!\")\n",
        "        logger.error(\"It should point to your local model directory or a Hugging Face Hub ID.\")\n",
        "        logger.error(\"=\"*80)\n",
        "        return False\n",
        "    \n",
        "    # Check if it's a local path\n",
        "    if not model_path.startswith(\"http\") and not \"/\" in model_path.split(\":\")[0]:\n",
        "        model_dir = Path(model_path)\n",
        "        if not model_dir.exists():\n",
        "            logger.warning(f\"Local model path does not exist: {model_path}\")\n",
        "            logger.warning(\"This might be a Hugging Face Hub model ID, which is fine.\")\n",
        "        else:\n",
        "            # Check for common model files\n",
        "            required_files = [\"config.json\", \"pytorch_model.bin\"]\n",
        "            missing_files = [f for f in required_files if not (model_dir / f).exists()]\n",
        "            if missing_files:\n",
        "                logger.warning(f\"Missing model files: {missing_files}\")\n",
        "    \n",
        "    logger.info(f\"Model path validated: {model_path}\")\n",
        "    return True\n",
        "\n",
        "# Validate the model path\n",
        "if not validate_model_path(MODEL_PATH):\n",
        "    raise ValueError(\"Invalid model path. Please update MODEL_PATH variable.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Benchmark Execution\n",
        "\n",
        "Run the benchmarks on all configured tasks. The evaluation is split into 0-shot and few-shot tasks for efficiency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "logger.info(f\"Starting benchmark for model: {MODEL_PATH}\")\n",
        "\n",
        "# Separate tasks into 0-shot and few-shot\n",
        "zero_shot_tasks = [t for t in TASKS if t not in FEW_SHOT_CONFIG]\n",
        "few_shot_tasks = {t: n for t, n in FEW_SHOT_CONFIG.items() if t in TASKS}\n",
        "\n",
        "# Prepare model arguments\n",
        "model_args = f\"pretrained={MODEL_PATH},trust_remote_code={TRUST_REMOTE_CODE}\"\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "# Run 0-shot tasks\n",
        "if zero_shot_tasks:\n",
        "    logger.info(\"\\n\" + \"=\"*50)\n",
        "    logger.info(f\"Running 0-shot tasks: {', '.join(zero_shot_tasks)}\")\n",
        "    logger.info(\"=\"*50)\n",
        "    \n",
        "    results_0_shot = simple_evaluate(\n",
        "        model=\"hf-causal\",\n",
        "        model_args=model_args,\n",
        "        tasks=zero_shot_tasks,\n",
        "        num_fewshot=0,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        # You can specify a device, e.g., device=\"cuda:0\"\n",
        "        # If not specified, it will auto-detect\n",
        "    )\n",
        "    all_results.update(results_0_shot['results'])\n",
        "\n",
        "# Run few-shot tasks\n",
        "for task_name, num_shots in few_shot_tasks.items():\n",
        "    logger.info(\"\\n\" + \"=\"*50)\n",
        "    logger.info(f\"Running {num_shots}-shot task: {task_name}\")\n",
        "    logger.info(\"=\"*50)\n",
        "    \n",
        "    results_few_shot = simple_evaluate(\n",
        "        model=\"hf-causal\",\n",
        "        model_args=model_args,\n",
        "        tasks=[task_name],  # simple_evaluate expects a list of tasks\n",
        "        num_fewshot=num_shots,\n",
        "        batch_size=BATCH_SIZE,\n",
        "    )\n",
        "    all_results.update(results_few_shot['results'])\n",
        "\n",
        "# Results are stored in all_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Processing and Display\n",
        "\n",
        "Process the benchmark results and display a summary of the main metrics for each task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_and_display_results(results):\n",
        "    \"\"\"\n",
        "    Process and display the benchmark results.\n",
        "    \"\"\"\n",
        "    logger.info(\"\\n\" + \"=\"*50)\n",
        "    logger.info(\"Benchmark complete!\")\n",
        "    logger.info(\"=\"*50)\n",
        "\n",
        "    # Save the full results to a file\n",
        "    with open(OUTPUT_PATH, \"w\") as f:\n",
        "        json.dump(results, f, indent=4)\n",
        "    logger.info(f\"Detailed results saved to: {OUTPUT_PATH}\")\n",
        "\n",
        "    # Print a summary of the main metrics\n",
        "    print(\"\\n--- Benchmark Summary ---\\n\")\n",
        "    \n",
        "    for task_name, metrics in results.items():\n",
        "        # Extract the main metric for each task\n",
        "        main_metric = \"N/A\"\n",
        "        if \"acc,none\" in metrics:\n",
        "            main_metric = f\"acc: {metrics['acc,none']:.4f}\"\n",
        "        elif \"acc_norm,none\" in metrics:\n",
        "            main_metric = f\"acc_norm: {metrics['acc_norm,none']:.4f}\"\n",
        "        elif \"exact_match,none\" in metrics:\n",
        "            main_metric = f\"exact_match: {metrics['exact_match,none']:.4f}\"\n",
        "        elif \"word_perplexity,none\" in metrics:\n",
        "            # For perplexity, lower is better\n",
        "            main_metric = f\"word_perplexity: {metrics['word_perplexity,none']:.4f}\"\n",
        "\n",
        "        print(f\"{task_name:<20}: {main_metric}\")\n",
        "\n",
        "    print(\"\\n--- End of Summary ---\\n\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Process and display results\n",
        "processed_results = process_and_display_results(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detailed Results Analysis\n",
        "\n",
        "Display more detailed information about the results, including additional metrics where available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_detailed_results(results):\n",
        "    \"\"\"\n",
        "    Display detailed results for each task.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Detailed Results ---\\n\")\n",
        "    \n",
        "    for task_name, metrics in results.items():\n",
        "        print(f\"Task: {task_name}\")\n",
        "        print(\"-\" * (len(task_name) + 6))\n",
        "        \n",
        "        # Display all available metrics\n",
        "        for metric_name, value in metrics.items():\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"  {metric_name}: {value:.6f}\")\n",
        "            else:\n",
        "                print(f\"  {metric_name}: {value}\")\n",
        "        \n",
        "        print()  # Empty line between tasks\n",
        "\n",
        "# Display detailed results\n",
        "display_detailed_results(processed_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Summary Table\n",
        "\n",
        "Create a formatted table of the main results for easy comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_results_table(results):\n",
        "    \"\"\"\n",
        "    Create a formatted table of the main results.\n",
        "    \"\"\"\n",
        "    table_data = []\n",
        "    \n",
        "    for task_name, metrics in results.items():\n",
        "        row = {\"Task\": task_name}\n",
        "        \n",
        "        # Extract main metric\n",
        "        if \"acc,none\" in metrics:\n",
        "            row[\"Main Metric\"] = \"Accuracy\"\n",
        "            row[\"Score\"] = f\"{metrics['acc,none']:.4f}\"\n",
        "        elif \"acc_norm,none\" in metrics:\n",
        "            row[\"Main Metric\"] = \"Normalized Accuracy\"\n",
        "            row[\"Score\"] = f\"{metrics['acc_norm,none']:.4f}\"\n",
        "        elif \"exact_match,none\" in metrics:\n",
        "            row[\"Main Metric\"] = \"Exact Match\"\n",
        "            row[\"Score\"] = f\"{metrics['exact_match,none']:.4f}\"\n",
        "        elif \"word_perplexity,none\" in metrics:\n",
        "            row[\"Main Metric\"] = \"Word Perplexity\"\n",
        "            row[\"Score\"] = f\"{metrics['word_perplexity,none']:.4f}\"\n",
        "        else:\n",
        "            row[\"Main Metric\"] = \"Other\"\n",
        "            row[\"Score\"] = \"N/A\"\n",
        "        \n",
        "        # Add few-shot info\n",
        "        if task_name in FEW_SHOT_CONFIG:\n",
        "            row[\"Few-Shot\"] = f\"{FEW_SHOT_CONFIG[task_name]}-shot\"\n",
        "        else:\n",
        "            row[\"Few-Shot\"] = \"0-shot\"\n",
        "        \n",
        "        table_data.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(table_data)\n",
        "    return df\n",
        "\n",
        "# Create and display the results table\n",
        "results_table = create_results_table(processed_results)\n",
        "print(\"\\n--- Results Summary Table ---\\n\")\n",
        "print(results_table.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "The benchmarking is complete! Here are some suggestions for next steps:\n",
        "\n",
        "1. **Compare with baselines**: Compare these results with standard GPT-2 or other transformer models\n",
        "2. **Analyze performance patterns**: Look for tasks where the MLA architecture performs particularly well or poorly\n",
        "3. **Hyperparameter tuning**: Experiment with different MLA configurations (q_shared_dim, kv_shared_dim, etc.)\n",
        "4. **Ablation studies**: Test the impact of different components of the MLA architecture\n",
        "5. **Scale experiments**: Test how performance changes with model size or training data\n",
        "\n",
        "The detailed results have been saved to `benchmark_results.json` for further analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
