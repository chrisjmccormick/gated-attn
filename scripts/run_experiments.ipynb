{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjmccormick/shared-subspaces/blob/main/subspace_decoder/scripts/run_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ip42SV3Jr39"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-A4hvT3JsjL"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5SFQDjNJtk2"
      },
      "source": [
        "This notebook demonstrates how to run the pre-training and fine-tuning scripts from a command line, and is also setup to allow you to run them from within the notebook.\n",
        "\n",
        "**Training Arguments**\n",
        "\n",
        "The scripts are designed such that everything is specified through `.json` config files rather than on the command line. However, there is a command line utility to define new configurations--see the \"Defining a New Run\" section at the end of this notebook.\n",
        "\n",
        "The examples below run one of the existing configurations.\n",
        "\n",
        "**Weights and Biases**\n",
        "\n",
        "The scripts are set up to log to wandb by default. You can change the `wandb_mode` variable below to 'offline' if you don't have an account / don't want to log online.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA-wKp3rYTB5"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyegsMa3mLhA"
      },
      "source": [
        "# S1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a flag we can use to determine if we are running within a Colab instance.\n",
        "is_colab = \"google.colab\" in str(get_ipython())"
      ],
      "metadata": {
        "id": "XVsyG-ca5HmB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fmU5PmGGjYa"
      },
      "source": [
        "## 1.1. Clone Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lg9vyvjEYD3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d85a02-e513-4a09-8e72-abd3821b39f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'shared-subspaces'...\n",
            "remote: Enumerating objects: 647, done.\u001b[K\n",
            "remote: Counting objects: 100% (384/384), done.\u001b[K\n",
            "remote: Compressing objects: 100% (223/223), done.\u001b[K\n",
            "remote: Total 647 (delta 244), reused 270 (delta 159), pack-reused 263 (from 1)\u001b[K\n",
            "Receiving objects: 100% (647/647), 11.06 MiB | 9.97 MiB/s, done.\n",
            "Resolving deltas: 100% (371/371), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/chrisjmccormick/shared-subspaces.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcmcny0IjIl3"
      },
      "source": [
        "Provide the full path to the subspace_decoder folder.\n",
        "\n",
        "This will be added to the PYTHONPATH when executing the scripts so that they can import the classes from the local files.\n",
        "\n",
        "This variable is also used to construct paths to config files and scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "em7Cu-VrjFRa"
      },
      "outputs": [],
      "source": [
        "# For Google Colab,\n",
        "base_path = \"/content/shared-subspaces/subspace_decoder\"\n",
        "\n",
        "# For Lambda / Ubuntu,\n",
        "#base_path = \"/home/ubuntu/shared-subspaces/subspace_decoder\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To switch to a development branch:"
      ],
      "metadata": {
        "id": "RqhrDbY_8_fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd shared-subspaces\n",
        "\n",
        "# Change to branch 'gpt-2-scale'\n",
        "!git checkout gpt-2-scale"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfNUeJtW876k",
        "outputId": "89fa7937-dc5b-459c-92fb-4ca42d5f1d1a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/shared-subspaces\n",
            "Branch 'gpt-2-scale' set up to track remote branch 'gpt-2-scale' from 'origin'.\n",
            "Switched to a new branch 'gpt-2-scale'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. FlashAttention on Colab"
      ],
      "metadata": {
        "id": "aECd8Frm0rgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FlashAttention does not come pre-installed on Colab instances, and is very time consuming to install manually because it has to be built from source.\n",
        "\n",
        "The below GitHub repo, however, provides pre-built wheels which make setup easy.\n",
        "\n",
        "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases\n",
        "\n",
        "The key is just to identify the correct wheel to use from the giant list.\n",
        "\n",
        "We need the wheel specific to our version of python, pytorch, and CUDA. So first we'll print those out:"
      ],
      "metadata": {
        "id": "sloj5Pzw3zVF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Hvoxy6VzQ-rk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef03707-1066-43d6-85c7-7118d79b0a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA: 12.6\n",
            "\n",
            "GPU: NVIDIA A100-SXM4-80GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA:\", torch.version.cuda)\n",
        "print(\"\")\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's difficult to find the correct wheel because they are all hidden underneath different releases, and searching the page doesn't work unless the releases are expanded.\n",
        "\n",
        "With some hunting, I was able to find the correct version for Colab's current configuration:"
      ],
      "metadata": {
        "id": "Eg990s034tIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This wheel is specific to Colab.\n",
        "if is_colab:\n",
        "    # Define the wheel details\n",
        "    WHEEL_URL = \"https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl\"\n",
        "    WHEEL_NAME = \"flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl\"\n",
        "\n",
        "    # Download and install the wheel\n",
        "    !wget {WHEEL_URL}\n",
        "    !pip install {WHEEL_NAME}\n",
        "\n",
        "    # Clean up the downloaded file\n",
        "    import os\n",
        "    os.remove(WHEEL_NAME)\n",
        "\n",
        "    print(\"\\n✅ FlashAttention 2 installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1sF6UYbyiT1",
        "outputId": "00c1ea5a-cea6-425e-ec8c-3eb7d54366b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-03 16:43:22--  https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.4.11/flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/878958395/9d318eca-33f7-4c0d-9b3f-6e90e52421f8?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-03T17%3A43%3A34Z&rscd=attachment%3B+filename%3Dflash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-03T16%3A42%3A47Z&ske=2025-10-03T17%3A43%3A34Z&sks=b&skv=2018-11-09&sig=Cw8AAOdQqzVHHx35ixkgLS9q60UQfUFb0Le6AM17ytE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTUxMzQwMiwibmJmIjoxNzU5NTA5ODAyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.afvaHCfDLgKYFgs-zK-1gZgmRoQ8drKnY24GJ59ZR70&response-content-disposition=attachment%3B%20filename%3Dflash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-03 16:43:23--  https://release-assets.githubusercontent.com/github-production-release-asset/878958395/9d318eca-33f7-4c0d-9b3f-6e90e52421f8?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-03T17%3A43%3A34Z&rscd=attachment%3B+filename%3Dflash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-03T16%3A42%3A47Z&ske=2025-10-03T17%3A43%3A34Z&sks=b&skv=2018-11-09&sig=Cw8AAOdQqzVHHx35ixkgLS9q60UQfUFb0Le6AM17ytE%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTUxMzQwMiwibmJmIjoxNzU5NTA5ODAyLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.afvaHCfDLgKYFgs-zK-1gZgmRoQ8drKnY24GJ59ZR70&response-content-disposition=attachment%3B%20filename%3Dflash_attn-2.8.3%2Bcu126torch2.8-cp312-cp312-linux_x86_64.whl&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 116893614 (111M) [application/octet-stream]\n",
            "Saving to: ‘flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl’\n",
            "\n",
            "flash_attn-2.8.3+cu 100%[===================>] 111.48M  7.04MB/s    in 14s     \n",
            "\n",
            "2025-10-03 16:43:37 (7.79 MB/s) - ‘flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl’ saved [116893614/116893614]\n",
            "\n",
            "Processing ./flash_attn-2.8.3+cu126torch2.8-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn==2.8.3+cu126torch2.8) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn==2.8.3+cu126torch2.8) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu126torch2.8) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn==2.8.3+cu126torch2.8) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn==2.8.3+cu126torch2.8) (3.0.3)\n",
            "Installing collected packages: flash-attn\n",
            "Successfully installed flash-attn-2.8.3\n",
            "\n",
            "✅ FlashAttention 2 installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4sOT95lInOc"
      },
      "source": [
        "## 1.3. Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfjqf9ynkjkq"
      },
      "source": [
        "To provide your wandb API key for the script:\n",
        "1. You could paste it in manually on the training command lines further down.\n",
        "2. Or, use the secrets panel (the key symbol on the left edge of the notebook) and:\n",
        "    * Define your wandb api key as `wandb_api_key`.\n",
        "    * Grant access to this notebook.\n",
        "    * Run the below cell to retrieve it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A4mf-KNWjEVZ"
      },
      "outputs": [],
      "source": [
        "# Set to false if you don't want to use wandb.\n",
        "# The scripts will still log using the wandb library, but to a local directory.\n",
        "use_wandb = True\n",
        "\n",
        "if use_wandb:\n",
        "\n",
        "    # Enable Weights & Biases logging (online mode)\n",
        "    wandb_mode = \"online\"\n",
        "\n",
        "    if is_colab:\n",
        "        # Get key from colab secrets\n",
        "        from google.colab import userdata\n",
        "\n",
        "        # Get wandb API key from Colab secrets\n",
        "        wandb_key = userdata.get(\"wandb_api_key\")\n",
        "    else:\n",
        "        # Get the key from the environment\n",
        "        # Import python dot env\n",
        "        from dotenv import load_dotenv\n",
        "\n",
        "        # Load the environment variables from the .env file\n",
        "        load_dotenv()\n",
        "\n",
        "        wandb_key = os.getenv(\"wandb_api_key\")\n",
        "\n",
        "# Set to offline if you don't want to log in.\n",
        "else:\n",
        "    wandb_mode = \"offline\"\n",
        "\n",
        "    wandb_key = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7koRYZ_X6AKa"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6UjLucskFCQ"
      },
      "source": [
        "# S2. Run a Single Config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Choose Config"
      ],
      "metadata": {
        "id": "POtW5xYC6DHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6Sp7iLw7wQ-",
        "outputId": "2bcb0ea4-908a-422b-ebca-48c63bc2f5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== /content/shared-subspaces/subspace_decoder/configs/tiny_mla_wiki103.json ========\n",
            "\n",
            "{\n",
            "    \"shorthand\": \"model.256.lyr.6 - seqlen.128 - mla.96.64.0 - ah8.32 - rd16.16\",\n",
            "    \"notes\": \"Tiny MLA baseline configuration trained on wikitext103\",\n",
            "    \"model\": {\n",
            "        \"hidden_size\": 256,\n",
            "        \"num_hidden_layers\": 6,\n",
            "        \"intermediate_size\": 672,\n",
            "        \"vocab_size\": 50257,\n",
            "        \"tie_word_embeddings\": true,\n",
            "        \"max_position_embeddings\": 128,\n",
            "        \"norm_type\": \"rmsnorm\",\n",
            "        \"layer_norm_eps\": 1e-12,\n",
            "        \"rms_norm_eps\": 1e-06,\n",
            "        \"num_dense_layers\": 0,\n",
            "        \"num_attention_heads\": 8,\n",
            "        \"q_shared_dim\": 96,\n",
            "        \"kv_shared_dim\": 64,\n",
            "        \"o_shared_dim\": null,\n",
            "        \"qk_private_dim\": 32,\n",
            "        \"vo_private_dim\": 32,\n",
            "        \"rope_dims\": 16,\n",
            "        \"nope_dims\": 16,\n",
            "        \"rope_theta\": 10000.0,\n",
            "        \"rope_scaling\": null,\n",
            "        \"attention_bias\": false,\n",
            "        \"attention_backend\": \"flash_attention_2\",\n",
            "        \"ffn_decompose\": false,\n",
            "        \"ffn_rank\": null,\n",
            "        \"vocab_subspace\": false,\n",
            "        \"vocab_rank\": null,\n",
            "        \"hidden_dropout_prob\": 0.1,\n",
            "        \"attention_dropout_prob\": 0.1,\n",
            "        \"classifier_dropout\": null,\n",
            "        \"initializer_range\": 0.02\n",
            "    },\n",
            "    \"pre_train\": {\n",
            "        \"wandb_project\": \"decoder-pretrain-wiki\",\n",
            "        \"output_dir\": \"checkpoints/tiny_mla_wiki_baseline\",\n",
            "        \"seed\": 42,\n",
            "        \"logging_steps\": 50,\n",
            "        \"save_steps\": 300,\n",
            "        \"train_batch_size\": 1024,\n",
            "        \"gradient_accumulation_steps\": 1,\n",
            "        \"learning_rate\": 0.0005,\n",
            "        \"num_train_steps\": 12500,\n",
            "        \"eval_steps\": 1000,\n",
            "        \"weight_decay\": 0.01,\n",
            "        \"num_workers\": 8,\n",
            "        \"pin_memory\": true,\n",
            "        \"dataset_name\": \"wikitext\",\n",
            "        \"dataset_config\": \"wikitext-103-raw-v1\",\n",
            "        \"dataset_subset_pct\": null,\n",
            "        \"max_seq_length\": 128,\n",
            "        \"eval_batch_size\": 64,\n",
            "        \"fp16\": false,\n",
            "        \"bf16\": true,\n",
            "        \"torch_compile\": true,\n",
            "        \"torch_compile_backend\": \"inductor\",\n",
            "        \"torch_compile_mode\": \"default\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "#  Choose which config file to run\n",
        "pretrain_config_path = f\"{base_path}/configs/tiny_mla_wiki103.json\"\n",
        "\n",
        "# Make sure it's a valid path\n",
        "if not os.path.exists(pretrain_config_path):\n",
        "    raise ValueError(f\"Config file {pretrain_config_path} does not exist.\")\n",
        "\n",
        "# Print it out.\n",
        "with open(pretrain_config_path, \"r\") as f:\n",
        "    pretrain_config = json.load(f)\n",
        "\n",
        "print(f\"\\n======== {pretrain_config_path} ========\\n\")\n",
        "\n",
        "# Print out the configuration with spacing.\n",
        "json_str = json.dumps(pretrain_config, indent=4)\n",
        "print(json_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVSazfr1kGw_"
      },
      "source": [
        "## 2.2. Run Pre-Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_09o6vrPir9K",
        "outputId": "1ee08ae5-0af5-46b5-caec-e33e1fca2dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======= Pre-Train ========\n",
            "\n",
            "TF available (Transformers thinks): False\n",
            "Importing Packages...\n",
            "\n",
            "PROJECT_ROOT /content/shared-subspaces/subspace_decoder\n",
            "model.256.lyr.6 - seqlen.128 - mla.96.64.0 - ah8.32 - rd16.16\n",
            "✓ BFloat16 is supported on this hardware\n",
            "✓ torch.compile enabled:\n",
            "  Backend: inductor\n",
            "  Mode: default\n",
            "  Note: First training step will be slower due to compilation.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchrismccormick\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "DatasetDict({\n",
            "    test: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 4358\n",
            "    })\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 1801350\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3760\n",
            "    })\n",
            "})\n",
            "Initializing model...\n",
            "config.q_shared_dim 96\n",
            "config.q_shared_dim 96\n",
            "config.q_shared_dim 96\n",
            "config.q_shared_dim 96\n",
            "config.q_shared_dim 96\n",
            "config.q_shared_dim 96\n",
            "SharedSpaceDecoderForCausalLM(\n",
            "  (model): SharedSpaceDecoderModel(\n",
            "    (vocab_embed): Embedding(50257, 256)\n",
            "    (rope): RotaryEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x SharedSpaceDecoderLayer(\n",
            "        (attn_input_norm): DeepseekV3RMSNorm()\n",
            "        (self_attn): MultiheadLatentAttention(\n",
            "          (q_shared_proj): Linear(in_features=256, out_features=96, bias=False)\n",
            "          (q_shared_norm): DeepseekV3RMSNorm()\n",
            "          (kv_shared_proj): Linear(in_features=256, out_features=64, bias=False)\n",
            "          (kv_shared_norm): DeepseekV3RMSNorm()\n",
            "          (q_private_proj): Linear(in_features=96, out_features=256, bias=False)\n",
            "          (kv_private_proj): Linear(in_features=64, out_features=512, bias=False)\n",
            "          (o_proj): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (ffn_input_norm): DeepseekV3RMSNorm()\n",
            "        (ffn): SubspaceFeedForward(\n",
            "          (W_in): Linear(in_features=256, out_features=672, bias=True)\n",
            "          (W_gate): Linear(in_features=256, out_features=672, bias=True)\n",
            "          (W_out): Linear(in_features=672, out_features=256, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm): DeepseekV3RMSNorm()\n",
            "  (lm_head): Linear(in_features=256, out_features=50257, bias=False)\n",
            ")\n",
            "\n",
            "======== Model ========\n",
            "SharedSpaceDecoderConfig {\n",
            "  \"attention_backend\": \"flash_attention_2\",\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"ffn_decompose\": false,\n",
            "  \"ffn_rank\": null,\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 672,\n",
            "  \"kv_shared_dim\": 64,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 128,\n",
            "  \"model_type\": \"shared_subspace_decoder\",\n",
            "  \"nope_dims\": 16,\n",
            "  \"norm_type\": \"rmsnorm\",\n",
            "  \"num_attention_heads\": 8,\n",
            "  \"num_dense_layers\": 0,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"o_shared_dim\": null,\n",
            "  \"q_shared_dim\": 96,\n",
            "  \"qk_private_dim\": 32,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_dims\": 16,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"transformers_version\": \"4.56.2\",\n",
            "  \"vo_private_dim\": 32,\n",
            "  \"vocab_rank\": null,\n",
            "  \"vocab_size\": 50257,\n",
            "  \"vocab_subspace\": false\n",
            "}\n",
            "\n",
            "\n",
            "======== Pre-Train ========\n",
            "{\n",
            "  \"wandb_project\": \"decoder-pretrain-wiki\",\n",
            "  \"output_dir\": \"checkpoints/tiny_mla_wiki_baseline\",\n",
            "  \"seed\": 42,\n",
            "  \"logging_steps\": 50,\n",
            "  \"save_steps\": 300,\n",
            "  \"train_batch_size\": 1024,\n",
            "  \"gradient_accumulation_steps\": 1,\n",
            "  \"learning_rate\": 0.0005,\n",
            "  \"num_train_steps\": 12500,\n",
            "  \"eval_steps\": 1000,\n",
            "  \"weight_decay\": 0.01,\n",
            "  \"num_workers\": 8,\n",
            "  \"pin_memory\": true,\n",
            "  \"dataset_name\": \"wikitext\",\n",
            "  \"dataset_config\": \"wikitext-103-raw-v1\",\n",
            "  \"dataset_subset_pct\": null,\n",
            "  \"max_seq_length\": 128,\n",
            "  \"eval_batch_size\": 64,\n",
            "  \"fp16\": false,\n",
            "  \"bf16\": true,\n",
            "  \"torch_compile\": true,\n",
            "  \"torch_compile_backend\": \"inductor\",\n",
            "  \"torch_compile_mode\": \"default\"\n",
            "}\n",
            "\n",
            "======== Batch Size Configuration ========\n",
            "Device batch size: 1024\n",
            "Gradient accumulation steps: 1\n",
            "Effective batch size: 1024\n",
            "=============================\n",
            "\n",
            "\n",
            "======== Parameters ========\n",
            "The model has 92 different named parameters.\n",
            "\n",
            "Total elements: 16.17M\n",
            "\n",
            "The model has 92 different named parameters.\n",
            "\n",
            "Total elements: 16.17M\n",
            "\n",
            "Parameter Name                                              Dimensions       Total Values    Trainable\n",
            "\n",
            "model.vocab_embed.weight                                    50,257 x 256           12.27M    True\n",
            "model.layers.0.attn_input_norm.weight                          256 x -               256     True\n",
            "model.layers.0.self_attn.q_shared_proj.weight                   96 x 256              24K    True\n",
            "model.layers.0.self_attn.q_shared_norm.weight                   96 x -                96     True\n",
            "model.layers.0.self_attn.kv_shared_proj.weight                  64 x 256              16K    True\n",
            "model.layers.0.self_attn.kv_shared_norm.weight                  64 x -                64     True\n",
            "model.layers.0.self_attn.q_private_proj.weight                 256 x 96               24K    True\n",
            "model.layers.0.self_attn.kv_private_proj.weight                512 x 64               32K    True\n",
            "model.layers.0.self_attn.o_proj.weight                         256 x 256              64K    True\n",
            "model.layers.0.ffn_input_norm.weight                           256 x -               256     True\n",
            "model.layers.0.ffn.W_in.weight                                 672 x 256             168K    True\n",
            "model.layers.0.ffn.W_in.bias                                   672 x -               672     True\n",
            "model.layers.0.ffn.W_gate.weight                               672 x 256             168K    True\n",
            "model.layers.0.ffn.W_gate.bias                                 672 x -               672     True\n",
            "model.layers.0.ffn.W_out.weight                                256 x 672             168K    True\n",
            "model.layers.0.ffn.W_out.bias                                  256 x -               256     True\n",
            "model.layers.1.attn_input_norm.weight                          256 x -               256     True\n",
            "model.layers.1.self_attn.q_shared_proj.weight                   96 x 256              24K    True\n",
            "model.layers.1.self_attn.q_shared_norm.weight                   96 x -                96     True\n",
            "model.layers.1.self_attn.kv_shared_proj.weight                  64 x 256              16K    True\n",
            "model.layers.1.self_attn.kv_shared_norm.weight                  64 x -                64     True\n",
            "model.layers.1.self_attn.q_private_proj.weight                 256 x 96               24K    True\n",
            "model.layers.1.self_attn.kv_private_proj.weight                512 x 64               32K    True\n",
            "model.layers.1.self_attn.o_proj.weight                         256 x 256              64K    True\n",
            "model.layers.1.ffn_input_norm.weight                           256 x -               256     True\n",
            "model.layers.1.ffn.W_in.weight                                 672 x 256             168K    True\n",
            "model.layers.1.ffn.W_in.bias                                   672 x -               672     True\n",
            "model.layers.1.ffn.W_gate.weight                               672 x 256             168K    True\n",
            "model.layers.1.ffn.W_gate.bias                                 672 x -               672     True\n",
            "model.layers.1.ffn.W_out.weight                                256 x 672             168K    True\n",
            "model.layers.1.ffn.W_out.bias                                  256 x -               256     True\n",
            "model.layers.2.attn_input_norm.weight                          256 x -               256     True\n",
            "model.layers.2.self_attn.q_shared_proj.weight                   96 x 256              24K    True\n",
            "model.layers.2.self_attn.q_shared_norm.weight                   96 x -                96     True\n",
            "model.layers.2.self_attn.kv_shared_proj.weight                  64 x 256              16K    True\n",
            "model.layers.2.self_attn.kv_shared_norm.weight                  64 x -                64     True\n",
            "model.layers.2.self_attn.q_private_proj.weight                 256 x 96               24K    True\n",
            "model.layers.2.self_attn.kv_private_proj.weight                512 x 64               32K    True\n",
            "model.layers.2.self_attn.o_proj.weight                         256 x 256              64K    True\n",
            "model.layers.2.ffn_input_norm.weight                           256 x -               256     True\n",
            "model.layers.2.ffn.W_in.weight                                 672 x 256             168K    True\n",
            "model.layers.2.ffn.W_in.bias                                   672 x -               672     True\n",
            "model.layers.2.ffn.W_gate.weight                               672 x 256             168K    True\n",
            "model.layers.2.ffn.W_gate.bias                                 672 x -               672     True\n",
            "model.layers.2.ffn.W_out.weight                                256 x 672             168K    True\n",
            "model.layers.2.ffn.W_out.bias                                  256 x -               256     True\n",
            "model.layers.3.attn_input_norm.weight                          256 x -               256     True\n",
            "model.layers.3.self_attn.q_shared_proj.weight                   96 x 256              24K    True\n",
            "model.layers.3.self_attn.q_shared_norm.weight                   96 x -                96     True\n",
            "model.layers.3.self_attn.kv_shared_proj.weight                  64 x 256              16K    True\n",
            "model.layers.3.self_attn.kv_shared_norm.weight                  64 x -                64     True\n",
            "model.layers.3.self_attn.q_private_proj.weight                 256 x 96               24K    True\n",
            "model.layers.3.self_attn.kv_private_proj.weight                512 x 64               32K    True\n",
            "model.layers.3.self_attn.o_proj.weight                         256 x 256              64K    True\n",
            "model.layers.3.ffn_input_norm.weight                           256 x -               256     True\n",
            "model.layers.3.ffn.W_in.weight                                 672 x 256             168K    True\n",
            "model.layers.3.ffn.W_in.bias                                   672 x -               672     True\n",
            "model.layers.3.ffn.W_gate.weight                               672 x 256             168K    True\n",
            "model.layers.3.ffn.W_gate.bias                                 672 x -               672     True\n",
            "model.layers.3.ffn.W_out.weight                                256 x 672             168K    True\n",
            "model.layers.3.ffn.W_out.bias                                  256 x -               256     True\n",
            "model.layers.4.attn_input_norm.weight                          256 x -               256     True\n",
            "model.layers.4.self_attn.q_shared_proj.weight                   96 x 256              24K    True\n",
            "model.layers.4.self_attn.q_shared_norm.weight                   96 x -                96     True\n",
            "model.layers.4.self_attn.kv_shared_proj.weight                  64 x 256              16K    True\n",
            "model.layers.4.self_attn.kv_shared_norm.weight                  64 x -                64     True\n",
            "model.layers.4.self_attn.q_private_proj.weight                 256 x 96               24K    True\n",
            "model.layers.4.self_attn.kv_private_proj.weight                512 x 64               32K    True\n",
            "model.layers.4.self_attn.o_proj.weight                         256 x 256              64K    True\n",
            "model.layers.4.ffn_input_norm.weight                           256 x -               256     True\n",
            "model.layers.4.ffn.W_in.weight                                 672 x 256             168K    True\n",
            "model.layers.4.ffn.W_in.bias                                   672 x -               672     True\n",
            "model.layers.4.ffn.W_gate.weight                               672 x 256             168K    True\n",
            "model.layers.4.ffn.W_gate.bias                                 672 x -               672     True\n",
            "model.layers.4.ffn.W_out.weight                                256 x 672             168K    True\n",
            "model.layers.4.ffn.W_out.bias                                  256 x -               256     True\n",
            "model.layers.5.attn_input_norm.weight                          256 x -               256     True\n",
            "model.layers.5.self_attn.q_shared_proj.weight                   96 x 256              24K    True\n",
            "model.layers.5.self_attn.q_shared_norm.weight                   96 x -                96     True\n",
            "model.layers.5.self_attn.kv_shared_proj.weight                  64 x 256              16K    True\n",
            "model.layers.5.self_attn.kv_shared_norm.weight                  64 x -                64     True\n",
            "model.layers.5.self_attn.q_private_proj.weight                 256 x 96               24K    True\n",
            "model.layers.5.self_attn.kv_private_proj.weight                512 x 64               32K    True\n",
            "model.layers.5.self_attn.o_proj.weight                         256 x 256              64K    True\n",
            "model.layers.5.ffn_input_norm.weight                           256 x -               256     True\n",
            "model.layers.5.ffn.W_in.weight                                 672 x 256             168K    True\n",
            "model.layers.5.ffn.W_in.bias                                   672 x -               672     True\n",
            "model.layers.5.ffn.W_gate.weight                               672 x 256             168K    True\n",
            "model.layers.5.ffn.W_gate.bias                                 672 x -               672     True\n",
            "model.layers.5.ffn.W_out.weight                                256 x 672             168K    True\n",
            "model.layers.5.ffn.W_out.bias                                  256 x -               256     True\n",
            "norm.weight                                                    256 x -               256     True\n",
            "\n",
            "Total elements: 16.17M\n",
            "\n",
            "16.17M - model.256.lyr.6 - seqlen.128 - mla.96.64.0 - ah8.32 - rd16.16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m setting up run sk13mctj (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m setting up run sk13mctj (0.2s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/shared-subspaces/wandb/run-20251003_170409-sk13mctj\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m16.17M - model.256.lyr.6 - seqlen.128 - mla.96.64.0 - ah8.32 - rd16.16\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/chrismccormick/decoder-pretrain-wiki\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/chrismccormick/decoder-pretrain-wiki/runs/sk13mctj\u001b[0m\n",
            "TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=True,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=8,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=4,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=1000,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0005,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=checkpoints/tiny_mla_wiki_baseline/runs/Oct03_17-04-11_a023145dfa24,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=50,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=12500,\n",
            "metric_for_best_model=eval_loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=checkpoints/tiny_mla_wiki_baseline,\n",
            "overwrite_output_dir=False,\n",
            "parallelism_config=None,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=1024,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=16.17M - model.256.lyr.6 - seqlen.128 - mla.96.64.0 - ah8.32 - rd16.16,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=False,\n",
            "save_steps=300,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torch_compile=True,\n",
            "torch_compile_backend=inductor,\n",
            "torch_compile_mode=default,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=1250,\n",
            "weight_decay=0.01,\n",
            ")\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 50256, 'bos_token_id': 50256, 'pad_token_id': 50256}.\n",
            "{'loss': 10.5955, 'grad_norm': 1.396417498588562, 'learning_rate': 1.96e-05, 'epoch': 0.05}\n",
            "{'loss': 9.8577, 'grad_norm': 1.3758093118667603, 'learning_rate': 3.960000000000001e-05, 'epoch': 0.11}\n",
            "{'loss': 9.0529, 'grad_norm': 1.3757116794586182, 'learning_rate': 5.96e-05, 'epoch': 0.16}\n",
            "{'loss': 8.2513, 'grad_norm': 1.2878737449645996, 'learning_rate': 7.960000000000001e-05, 'epoch': 0.22}\n",
            "{'loss': 7.5025, 'grad_norm': 1.0634371042251587, 'learning_rate': 9.96e-05, 'epoch': 0.27}\n",
            "{'loss': 6.9042, 'grad_norm': 0.5876590609550476, 'learning_rate': 0.00011960000000000001, 'epoch': 0.33}\n",
            "{'loss': 6.4882, 'grad_norm': 0.9959695339202881, 'learning_rate': 0.0001396, 'epoch': 0.38}\n",
            "{'loss': 6.1947, 'grad_norm': 1.4186605215072632, 'learning_rate': 0.0001596, 'epoch': 0.44}\n",
            "{'loss': 5.9693, 'grad_norm': 0.9343092441558838, 'learning_rate': 0.0001796, 'epoch': 0.49}\n",
            "{'loss': 5.7909, 'grad_norm': 1.0474326610565186, 'learning_rate': 0.0001996, 'epoch': 0.55}\n",
            "{'loss': 5.6374, 'grad_norm': 1.2120380401611328, 'learning_rate': 0.0002196, 'epoch': 0.6}\n",
            "{'loss': 5.508, 'grad_norm': 1.5774863958358765, 'learning_rate': 0.00023960000000000002, 'epoch': 0.66}\n",
            "{'loss': 5.3894, 'grad_norm': 1.0493831634521484, 'learning_rate': 0.0002596, 'epoch': 0.71}\n",
            "{'loss': 5.2878, 'grad_norm': 1.4317682981491089, 'learning_rate': 0.0002796, 'epoch': 0.77}\n",
            "{'loss': 5.188, 'grad_norm': 1.6758582592010498, 'learning_rate': 0.00029959999999999996, 'epoch': 0.82}\n",
            "{'loss': 5.0954, 'grad_norm': 1.0458438396453857, 'learning_rate': 0.0003196, 'epoch': 0.88}\n",
            "{'loss': 5.0173, 'grad_norm': 1.0564541816711426, 'learning_rate': 0.0003396, 'epoch': 0.93}\n",
            "{'loss': 4.939, 'grad_norm': 1.0976901054382324, 'learning_rate': 0.00035959999999999996, 'epoch': 0.99}\n",
            "{'loss': 4.8613, 'grad_norm': 1.088515043258667, 'learning_rate': 0.0003796, 'epoch': 1.04}\n",
            "{'loss': 4.7867, 'grad_norm': 1.0357722043991089, 'learning_rate': 0.0003996, 'epoch': 1.1}\n",
            "  8% 1000/12500 [04:36<42:23,  4.52it/s]\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/31 [00:00<00:02, 12.36it/s]\u001b[A\n",
            " 16% 5/31 [00:00<00:01, 19.86it/s]\u001b[A\n",
            " 26% 8/31 [00:00<00:00, 23.18it/s]\u001b[A\n",
            " 35% 11/31 [00:00<00:00, 25.06it/s]\u001b[A\n",
            " 45% 14/31 [00:00<00:00, 26.23it/s]\u001b[A\n",
            " 55% 17/31 [00:00<00:00, 26.78it/s]\u001b[A\n",
            " 65% 20/31 [00:00<00:00, 27.31it/s]\u001b[A\n",
            " 74% 23/31 [00:00<00:00, 27.69it/s]\u001b[A\n",
            " 84% 26/31 [00:01<00:00, 27.96it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 4.732095400342855, 'eval_perplexity': 113.5332108062253, 'eval_runtime': 1.8274, 'eval_samples_per_second': 1071.989, 'eval_steps_per_second': 16.964, 'epoch': 1.1}\n",
            "  8% 1000/12500 [04:38<42:23,  4.52it/s]\n",
            "100% 31/31 [00:01<00:00, 28.14it/s]\u001b[A\n",
            "{'loss': 4.7226, 'grad_norm': 1.1036317348480225, 'learning_rate': 0.0004196, 'epoch': 1.15}\n",
            "{'loss': 4.6675, 'grad_norm': 0.911784827709198, 'learning_rate': 0.0004396, 'epoch': 1.2}\n",
            "{'loss': 4.61, 'grad_norm': 0.7894942760467529, 'learning_rate': 0.0004596, 'epoch': 1.26}\n",
            "{'loss': 4.5576, 'grad_norm': 0.9743266105651855, 'learning_rate': 0.00047960000000000006, 'epoch': 1.31}\n",
            "{'loss': 4.5046, 'grad_norm': 0.8220658302307129, 'learning_rate': 0.0004996, 'epoch': 1.37}\n",
            "{'loss': 4.464, 'grad_norm': 0.7614052295684814, 'learning_rate': 0.0004978222222222223, 'epoch': 1.42}\n",
            "{'loss': 4.4217, 'grad_norm': 0.8258338570594788, 'learning_rate': 0.0004956, 'epoch': 1.48}\n",
            "{'loss': 4.3807, 'grad_norm': 0.8556699752807617, 'learning_rate': 0.0004933777777777779, 'epoch': 1.53}\n",
            "{'loss': 4.3465, 'grad_norm': 0.716994047164917, 'learning_rate': 0.0004911555555555556, 'epoch': 1.59}\n",
            "{'loss': 4.3159, 'grad_norm': 0.8812611699104309, 'learning_rate': 0.0004889333333333334, 'epoch': 1.64}\n",
            "{'loss': 4.2823, 'grad_norm': 0.7830649614334106, 'learning_rate': 0.0004867111111111111, 'epoch': 1.7}\n",
            "{'loss': 4.2511, 'grad_norm': 0.7304012775421143, 'learning_rate': 0.0004844888888888889, 'epoch': 1.75}\n",
            "{'loss': 4.2344, 'grad_norm': 0.7071873545646667, 'learning_rate': 0.00048226666666666666, 'epoch': 1.81}\n",
            "{'loss': 4.2047, 'grad_norm': 0.7408241033554077, 'learning_rate': 0.00048004444444444446, 'epoch': 1.86}\n",
            "{'loss': 4.1858, 'grad_norm': 0.6445260643959045, 'learning_rate': 0.0004778222222222222, 'epoch': 1.92}\n",
            "{'loss': 4.166, 'grad_norm': 0.6422564387321472, 'learning_rate': 0.0004756, 'epoch': 1.97}\n",
            "{'loss': 4.1318, 'grad_norm': 0.7165148258209229, 'learning_rate': 0.0004733777777777778, 'epoch': 2.03}\n",
            "{'loss': 4.1085, 'grad_norm': 0.619766116142273, 'learning_rate': 0.00047115555555555556, 'epoch': 2.08}\n",
            "{'loss': 4.0871, 'grad_norm': 0.6287556886672974, 'learning_rate': 0.0004689333333333333, 'epoch': 2.14}\n",
            "{'loss': 4.0745, 'grad_norm': 0.6489611864089966, 'learning_rate': 0.00046671111111111117, 'epoch': 2.19}\n",
            " 16% 2000/12500 [08:23<38:47,  4.51it/s]\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 3/31 [00:00<00:00, 28.24it/s]\u001b[A\n",
            " 19% 6/31 [00:00<00:00, 27.96it/s]\u001b[A\n",
            " 29% 9/31 [00:00<00:00, 27.97it/s]\u001b[A\n",
            " 39% 12/31 [00:00<00:00, 27.97it/s]\u001b[A\n",
            " 48% 15/31 [00:00<00:00, 27.78it/s]\u001b[A\n",
            " 58% 18/31 [00:00<00:00, 27.86it/s]\u001b[A\n",
            " 68% 21/31 [00:00<00:00, 28.09it/s]\u001b[A\n",
            " 77% 24/31 [00:00<00:00, 28.23it/s]\u001b[A\n",
            " 87% 27/31 [00:00<00:00, 28.37it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 4.0710332532341145, 'eval_perplexity': 58.617498031704145, 'eval_runtime': 1.4829, 'eval_samples_per_second': 1321.063, 'eval_steps_per_second': 20.905, 'epoch': 2.19}\n",
            " 16% 2000/12500 [08:24<38:47,  4.51it/s]\n",
            "100% 31/31 [00:01<00:00, 28.42it/s]\u001b[A\n",
            "{'loss': 4.0618, 'grad_norm': 0.6908773183822632, 'learning_rate': 0.0004644888888888889, 'epoch': 2.25}\n",
            "{'loss': 4.05, 'grad_norm': 0.8249801397323608, 'learning_rate': 0.00046226666666666666, 'epoch': 2.3}\n",
            "{'loss': 4.0413, 'grad_norm': 0.7019258737564087, 'learning_rate': 0.0004600444444444444, 'epoch': 2.35}\n",
            "{'loss': 4.0268, 'grad_norm': 0.6474881172180176, 'learning_rate': 0.00045782222222222227, 'epoch': 2.41}\n",
            "{'loss': 4.0196, 'grad_norm': 0.6629468202590942, 'learning_rate': 0.0004556, 'epoch': 2.46}\n",
            "{'loss': 4.0033, 'grad_norm': 0.6052170991897583, 'learning_rate': 0.00045337777777777776, 'epoch': 2.52}\n",
            "{'loss': 3.9912, 'grad_norm': 0.7219094038009644, 'learning_rate': 0.00045115555555555557, 'epoch': 2.57}\n",
            "{'loss': 3.9809, 'grad_norm': 0.6564334034919739, 'learning_rate': 0.00044893333333333337, 'epoch': 2.63}\n",
            "{'loss': 3.9786, 'grad_norm': 0.5622611045837402, 'learning_rate': 0.0004467111111111111, 'epoch': 2.68}\n",
            "{'loss': 3.9656, 'grad_norm': 0.5956437587738037, 'learning_rate': 0.00044448888888888886, 'epoch': 2.74}\n",
            "{'loss': 3.9524, 'grad_norm': 0.6688022613525391, 'learning_rate': 0.00044226666666666667, 'epoch': 2.79}\n",
            "{'loss': 3.9424, 'grad_norm': 0.617857813835144, 'learning_rate': 0.00044004444444444447, 'epoch': 2.85}\n",
            "{'loss': 3.9345, 'grad_norm': 0.6172365546226501, 'learning_rate': 0.0004378222222222222, 'epoch': 2.9}\n",
            "{'loss': 3.9268, 'grad_norm': 0.5788294076919556, 'learning_rate': 0.0004356, 'epoch': 2.96}\n",
            "{'loss': 3.9103, 'grad_norm': 0.708136260509491, 'learning_rate': 0.00043337777777777776, 'epoch': 3.01}\n",
            "{'loss': 3.8782, 'grad_norm': 0.7501764893531799, 'learning_rate': 0.00043115555555555557, 'epoch': 3.07}\n",
            "{'loss': 3.8804, 'grad_norm': 0.6271185278892517, 'learning_rate': 0.00042893333333333337, 'epoch': 3.12}\n",
            "{'loss': 3.8745, 'grad_norm': 0.7225857377052307, 'learning_rate': 0.0004267111111111111, 'epoch': 3.18}\n",
            "{'loss': 3.8662, 'grad_norm': 0.6332756876945496, 'learning_rate': 0.0004244888888888889, 'epoch': 3.23}\n",
            "{'loss': 3.8613, 'grad_norm': 0.6871671080589294, 'learning_rate': 0.00042226666666666667, 'epoch': 3.29}\n",
            " 24% 3000/12500 [12:09<35:10,  4.50it/s]\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:00, 30.48it/s]\u001b[A\n",
            " 26% 8/31 [00:00<00:00, 28.24it/s]\u001b[A\n",
            " 35% 11/31 [00:00<00:00, 27.85it/s]\u001b[A\n",
            " 45% 14/31 [00:00<00:00, 27.71it/s]\u001b[A\n",
            " 55% 17/31 [00:00<00:00, 27.76it/s]\u001b[A\n",
            " 65% 20/31 [00:00<00:00, 27.74it/s]\u001b[A\n",
            " 74% 23/31 [00:00<00:00, 27.71it/s]\u001b[A\n",
            " 84% 26/31 [00:00<00:00, 27.70it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 3.8795168385465026, 'eval_perplexity': 48.400824008491846, 'eval_runtime': 1.4828, 'eval_samples_per_second': 1321.164, 'eval_steps_per_second': 20.907, 'epoch': 3.29}\n",
            " 24% 3000/12500 [12:10<35:10,  4.50it/s]\n",
            "100% 31/31 [00:01<00:00, 27.76it/s]\u001b[A\n",
            "{'loss': 3.8608, 'grad_norm': 0.6132106781005859, 'learning_rate': 0.00042004444444444447, 'epoch': 3.34}\n",
            "{'loss': 3.8561, 'grad_norm': 0.6262069344520569, 'learning_rate': 0.0004178222222222222, 'epoch': 3.4}\n",
            "{'loss': 3.8546, 'grad_norm': 0.6914224028587341, 'learning_rate': 0.0004156, 'epoch': 3.45}\n",
            "{'loss': 3.847, 'grad_norm': 0.6141162514686584, 'learning_rate': 0.0004133777777777778, 'epoch': 3.5}\n",
            "{'loss': 3.8379, 'grad_norm': 0.6134160757064819, 'learning_rate': 0.00041115555555555557, 'epoch': 3.56}\n",
            "{'loss': 3.8315, 'grad_norm': 0.7458935976028442, 'learning_rate': 0.0004089333333333333, 'epoch': 3.61}\n",
            "{'loss': 3.8328, 'grad_norm': 0.6316987872123718, 'learning_rate': 0.0004067111111111111, 'epoch': 3.67}\n",
            "{'loss': 3.8283, 'grad_norm': 0.689307451248169, 'learning_rate': 0.0004044888888888889, 'epoch': 3.72}\n",
            "{'loss': 3.8201, 'grad_norm': 0.604720413684845, 'learning_rate': 0.00040226666666666667, 'epoch': 3.78}\n",
            "{'loss': 3.8175, 'grad_norm': 0.6995941400527954, 'learning_rate': 0.0004000444444444444, 'epoch': 3.83}\n",
            "{'loss': 3.809, 'grad_norm': 0.6646829843521118, 'learning_rate': 0.00039782222222222227, 'epoch': 3.89}\n",
            "{'loss': 3.8072, 'grad_norm': 0.6304556727409363, 'learning_rate': 0.0003956, 'epoch': 3.94}\n",
            "{'loss': 3.803, 'grad_norm': 0.6218410134315491, 'learning_rate': 0.00039337777777777777, 'epoch': 4.0}\n",
            "{'loss': 3.7652, 'grad_norm': 0.6478456854820251, 'learning_rate': 0.0003911555555555555, 'epoch': 4.05}\n",
            "{'loss': 3.7685, 'grad_norm': 0.6091537475585938, 'learning_rate': 0.00038893333333333337, 'epoch': 4.11}\n",
            "{'loss': 3.7666, 'grad_norm': 0.6555205583572388, 'learning_rate': 0.0003867111111111111, 'epoch': 4.16}\n",
            "{'loss': 3.7658, 'grad_norm': 0.6149347424507141, 'learning_rate': 0.00038448888888888887, 'epoch': 4.22}\n",
            "{'loss': 3.763, 'grad_norm': 0.606968879699707, 'learning_rate': 0.00038226666666666667, 'epoch': 4.27}\n",
            "{'loss': 3.7612, 'grad_norm': 0.6035925149917603, 'learning_rate': 0.00038004444444444447, 'epoch': 4.33}\n",
            "{'loss': 3.7547, 'grad_norm': 0.5253604650497437, 'learning_rate': 0.0003778222222222222, 'epoch': 4.38}\n",
            " 32% 4000/12500 [15:55<31:25,  4.51it/s]\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:00, 30.83it/s]\u001b[A\n",
            " 26% 8/31 [00:00<00:00, 29.38it/s]\u001b[A\n",
            " 35% 11/31 [00:00<00:00, 29.06it/s]\u001b[A\n",
            " 45% 14/31 [00:00<00:00, 28.84it/s]\u001b[A\n",
            " 55% 17/31 [00:00<00:00, 28.78it/s]\u001b[A\n",
            " 65% 20/31 [00:00<00:00, 28.68it/s]\u001b[A\n",
            " 74% 23/31 [00:00<00:00, 28.68it/s]\u001b[A\n",
            " 84% 26/31 [00:00<00:00, 28.54it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 3.7882045845466914, 'eval_perplexity': 44.17701293897142, 'eval_runtime': 1.4693, 'eval_samples_per_second': 1333.323, 'eval_steps_per_second': 21.099, 'epoch': 4.38}\n",
            " 32% 4000/12500 [15:57<31:25,  4.51it/s]\n",
            "100% 31/31 [00:01<00:00, 28.43it/s]\u001b[A\n",
            "{'loss': 3.7526, 'grad_norm': 0.6982038021087646, 'learning_rate': 0.0003756, 'epoch': 4.44}\n",
            "{'loss': 3.7474, 'grad_norm': 0.5816389322280884, 'learning_rate': 0.00037337777777777777, 'epoch': 4.49}\n",
            "{'loss': 3.748, 'grad_norm': 0.588545024394989, 'learning_rate': 0.00037115555555555557, 'epoch': 4.55}\n",
            "{'loss': 3.7436, 'grad_norm': 0.6224834322929382, 'learning_rate': 0.0003689333333333333, 'epoch': 4.6}\n",
            "{'loss': 3.7438, 'grad_norm': 0.6880019903182983, 'learning_rate': 0.0003667111111111111, 'epoch': 4.65}\n",
            "{'loss': 3.7383, 'grad_norm': 0.5813742280006409, 'learning_rate': 0.00036448888888888887, 'epoch': 4.71}\n",
            "{'loss': 3.741, 'grad_norm': 0.5469034314155579, 'learning_rate': 0.00036226666666666667, 'epoch': 4.76}\n",
            "{'loss': 3.7377, 'grad_norm': 0.6110716462135315, 'learning_rate': 0.0003600444444444445, 'epoch': 4.82}\n",
            "{'loss': 3.7367, 'grad_norm': 0.642320454120636, 'learning_rate': 0.0003578222222222222, 'epoch': 4.87}\n",
            "{'loss': 3.7301, 'grad_norm': 0.6507238745689392, 'learning_rate': 0.0003556, 'epoch': 4.93}\n",
            "{'loss': 3.7283, 'grad_norm': 0.6190651059150696, 'learning_rate': 0.00035337777777777777, 'epoch': 4.98}\n",
            "{'loss': 3.7041, 'grad_norm': 0.6581182479858398, 'learning_rate': 0.0003511555555555556, 'epoch': 5.04}\n",
            "{'loss': 3.6944, 'grad_norm': 0.6409561038017273, 'learning_rate': 0.0003489333333333333, 'epoch': 5.09}\n",
            "{'loss': 3.6934, 'grad_norm': 0.6079127788543701, 'learning_rate': 0.0003467111111111111, 'epoch': 5.15}\n",
            "{'loss': 3.699, 'grad_norm': 0.59782475233078, 'learning_rate': 0.0003444888888888889, 'epoch': 5.2}\n",
            "{'loss': 3.6964, 'grad_norm': 0.6362614035606384, 'learning_rate': 0.0003422666666666667, 'epoch': 5.26}\n",
            "{'loss': 3.689, 'grad_norm': 0.6065399050712585, 'learning_rate': 0.0003400444444444444, 'epoch': 5.31}\n",
            "{'loss': 3.6926, 'grad_norm': 0.5543004274368286, 'learning_rate': 0.0003378222222222223, 'epoch': 5.37}\n",
            "{'loss': 3.6922, 'grad_norm': 0.5578762292861938, 'learning_rate': 0.0003356, 'epoch': 5.42}\n",
            "{'loss': 3.6852, 'grad_norm': 0.6346825361251831, 'learning_rate': 0.0003333777777777778, 'epoch': 5.48}\n",
            " 40% 5000/12500 [19:42<27:43,  4.51it/s]\n",
            "  0% 0/31 [00:00<?, ?it/s]\u001b[A\n",
            " 13% 4/31 [00:00<00:00, 30.06it/s]\u001b[A\n",
            " 26% 8/31 [00:00<00:00, 28.65it/s]\u001b[A\n",
            " 35% 11/31 [00:00<00:00, 28.32it/s]\u001b[A\n",
            " 45% 14/31 [00:00<00:00, 28.07it/s]\u001b[A\n",
            " 55% 17/31 [00:00<00:00, 28.06it/s]\u001b[A\n",
            " 65% 20/31 [00:00<00:00, 27.94it/s]\u001b[A\n",
            " 74% 23/31 [00:00<00:00, 28.07it/s]\u001b[A\n",
            " 84% 26/31 [00:00<00:00, 28.12it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 3.7259369799893887, 'eval_perplexity': 41.5101086701858, 'eval_runtime': 1.4983, 'eval_samples_per_second': 1307.501, 'eval_steps_per_second': 20.69, 'epoch': 5.48}\n",
            " 40% 5000/12500 [19:43<27:43,  4.51it/s]\n",
            "100% 31/31 [00:01<00:00, 28.25it/s]\u001b[A\n",
            "{'loss': 3.6871, 'grad_norm': 0.5958482027053833, 'learning_rate': 0.0003311555555555555, 'epoch': 5.53}\n",
            "{'loss': 3.6839, 'grad_norm': 0.5906832218170166, 'learning_rate': 0.0003289333333333334, 'epoch': 5.59}\n",
            "{'loss': 3.6876, 'grad_norm': 0.5951443910598755, 'learning_rate': 0.0003267111111111111, 'epoch': 5.64}\n",
            "{'loss': 3.6801, 'grad_norm': 0.5622800588607788, 'learning_rate': 0.0003244888888888889, 'epoch': 5.7}\n",
            "{'loss': 3.6853, 'grad_norm': 0.5960748195648193, 'learning_rate': 0.0003222666666666667, 'epoch': 5.75}\n",
            "{'loss': 3.6816, 'grad_norm': 0.6064601540565491, 'learning_rate': 0.0003200444444444445, 'epoch': 5.81}\n",
            "{'loss': 3.6804, 'grad_norm': 0.6007717251777649, 'learning_rate': 0.0003178222222222222, 'epoch': 5.86}\n",
            "{'loss': 3.6778, 'grad_norm': 0.550554096698761, 'learning_rate': 0.0003156, 'epoch': 5.91}\n",
            " 43% 5419/12500 [21:17<26:15,  4.50it/s]"
          ]
        }
      ],
      "source": [
        "print(\"\\n======= Pre-Train ========\\n\")\n",
        "\n",
        "# Construct the command line\n",
        "train_command = (\n",
        "    f\"TRANSFORMERS_NO_TF=1 \"\n",
        "    f\"PYTHONPATH={base_path} \"\n",
        "    f\"WANDB_MODE={wandb_mode} \"\n",
        "    f'WANDB_API_KEY=\"{wandb_key}\" '\n",
        "    f\"python {base_path}/scripts/train.py --config {pretrain_config_path}\"\n",
        ")\n",
        "\n",
        "# Run pre-training\n",
        "!{train_command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHVezQcela9q"
      },
      "source": [
        "**Optional - Save the checkpoint to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3eW0xow4lc1h",
        "outputId": "0e5b8a2e-9015-4379-de94-70ac36e404e4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/encoder-pretrain-wiki103/checkpoints'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "if False:\n",
        "    # Copy whatever's in the checkpoints folder over to Google Drive.\n",
        "    shutil.copytree(\n",
        "        \"/content/checkpoints\",\n",
        "        \"/content/drive/MyDrive/decoder-pretrain-wiki103/checkpoints\",\n",
        "        dirs_exist_ok = True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB-niCjY3efa"
      },
      "source": [
        "##  2.3. Fine-Tune SST-2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# =================\n",
        "#    SFT Config\n",
        "# =================\n",
        "\n",
        "# Specify the path to the SFT config and the pre-trained model config\n",
        "sft_config_path = f\"{base_path}/configs/fine-tune-sst2.json\"\n",
        "\n",
        "# Make sure paths are valid\n",
        "if not os.path.exists(sft_config_path):\n",
        "    raise ValueError(f\"SFT config file {sft_config_path} does not exist.\")\n",
        "\n",
        "# Print out the SFT configuration\n",
        "with open(sft_config_path, \"r\") as f:\n",
        "    sft_config = json.load(f)\n",
        "\n",
        "print(f\"\\n======== SFT Config: {sft_config_path} ========\\n\")\n",
        "json_str = json.dumps(sft_config, indent=4)\n",
        "print(json_str)\n",
        "\n",
        "# ================================\n",
        "#     Pretrained Model Config\n",
        "# ================================\n",
        "\n",
        "# Retrieve the config from the pre-training result\n",
        "\n",
        "# Verify the path still exists.\n",
        "if not os.path.exists(pretrain_config_path):\n",
        "    raise ValueError(f\"Pre-training config file {pretrain_config_path} does not exist.\")\n",
        "\n",
        "# Load the original pre-training configuration,\n",
        "with open(pretrain_config_path, \"r\") as f:\n",
        "    pretrain_config = json.load(f)\n",
        "\n",
        "# Then retrieve the path to the resulting `full_config.json` based on\n",
        "# the training output directory.\n",
        "saved_model_cfg_path = pretrain_config[\"pre_train\"][\"output_dir\"] + '/full_config.json'\n",
        "\n",
        "# Review the updated configuration.\n",
        "print(f\"\\n======== Config From Saved Model: {saved_model_cfg_path} ========\\n\")\n",
        "with open(saved_model_cfg_path, \"r\") as f:\n",
        "    print(json.dumps(json.load(f), indent=4))\n",
        "\n",
        "# ===================================\n",
        "#         Run fine-tuning\n",
        "# ===================================\n",
        "print(\"\\n======= Fine-Tune on SST-2 ========\\n\")\n",
        "\n",
        "# Construct the command line with separate config files\n",
        "train_command = (\n",
        "    f\"TRANSFORMERS_NO_TF=1 \"\n",
        "    f\"PYTHONPATH={base_path} \"\n",
        "    f\"WANDB_MODE={wandb_mode} \"\n",
        "    f'WANDB_API_KEY=\"{wandb_key}\" '\n",
        "    f\"python {base_path}/scripts/finetune_sst2.py --sft_config {sft_config_path} --model_config {saved_model_cfg_path}\"\n",
        ")\n",
        "\n",
        "# Run fine-tuning\n",
        "!{train_command}"
      ],
      "metadata": {
        "id": "sZKuNb4kEi6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPfIYtJUhU6M"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S3. Run Multiple Configs"
      ],
      "metadata": {
        "id": "sEE2zBwx6ijG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below cell combines the above, allowing for running multiple experiments.\n",
        "\n",
        "> Note: My experience has been that any run that takes an hour or more seems to be impractical on Colab, even with Colab Pro. My Notebook usually gets sluggish and eventually crashes. This may be because there is too much being printed to the output cell, however?"
      ],
      "metadata": {
        "id": "zmUQ1aqG7Ojj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrain_config_paths = [\n",
        "    f\"{base_path}/configs/gpt-2_mha.json\",\n",
        "    f\"{base_path}/configs/gpt-2_mla_192.96.192.json\",\n",
        "    f\"{base_path}/configs/gpt-2_mla_192.96.0.json\",\n",
        "    f\"{base_path}/configs/gpt-2_mla_0.96.192.json\",\n",
        "]\n",
        "\n",
        "# Quickly validate the fileneames.\n",
        "for pretrain_config_path in pretrain_config_paths:\n",
        "    if not os.path.exists(pretrain_config_path):\n",
        "        raise ValueError(f\"Model config file {pretrain_config_path} does not exist.\")\n",
        "\n",
        "\n",
        "# For each of the pre-training configurations...\n",
        "for pretrain_config_path in pretrain_config_paths:\n",
        "\n",
        "    # ===========================\n",
        "    #      Run pre-training\n",
        "    # ===========================\n",
        "    print(\"\\n======= Pre-Train ========\\n\")\n",
        "\n",
        "    # Construct the command line\n",
        "    #  > train_log.txt 2>&1\"\n",
        "    train_command = (\n",
        "        f\"TRANSFORMERS_NO_TF=1 \"\n",
        "        f\"PYTHONPATH={base_path} \"\n",
        "        f\"WANDB_MODE={wandb_mode} \"\n",
        "        f'WANDB_API_KEY=\"{wandb_key}\" '\n",
        "        f\"python {base_path}/scripts/train.py --config {pretrain_config_path}\"\n",
        "    )\n",
        "\n",
        "    # Run pre-training\n",
        "    !{train_command}\n",
        "\n",
        "\n",
        "    # ===================================\n",
        "    #    Retrieve the updated config\n",
        "    # ===================================\n",
        "    with open(pretrain_config_path, \"r\") as f:\n",
        "        pretrain_config = json.load(f)\n",
        "\n",
        "    saved_model_cfg_path = pretrain_config[\"pre_train\"][\"output_dir\"] + '/full_config.json'\n",
        "\n",
        "    print(f\"\\n======== Config From Saved Model: {saved_model_cfg_path} ========\\n\")\n",
        "    with open(saved_model_cfg_path, \"r\") as f:\n",
        "        print(json.dumps(json.load(f), indent=4))\n",
        "\n",
        "\n",
        "    # ===================================\n",
        "    #         Run fine-tuning\n",
        "    # ===================================\n",
        "    print(\"\\n======= Fine-Tune on SST-2 ========\\n\")\n",
        "\n",
        "    # Construct the command line with separate config files\n",
        "    train_command = (\n",
        "        f\"TRANSFORMERS_NO_TF=1 \"\n",
        "        f\"PYTHONPATH={base_path} \"\n",
        "        f\"WANDB_MODE={wandb_mode} \"\n",
        "        f'WANDB_API_KEY=\"{wandb_key}\" '\n",
        "        f\"python {base_path}/scripts/finetune_sst2.py --sft_config {sft_config_path} --model_config {saved_model_cfg_path}\"\n",
        "    )\n",
        "\n",
        "    # Run fine-tuning\n",
        "    !{train_command}\n"
      ],
      "metadata": {
        "id": "3QOIaD_E6ncS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eTIno8r6niE"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "Qtsh4Pfh7-nE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SagN8mgICj-g"
      },
      "source": [
        "## Defining a New Run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHYgedWDcd5"
      },
      "source": [
        "To modify the parameters from the command line, I created a command line utiltity in `/configs/create_new_config.py` which will copy one of the existing config files and allow you to specify any parameter changes.\n",
        "\n",
        "See the [script](https://github.com/chrisjmccormick/shared-subspaces/blob/main/subspace_encoder/configs/create_new_config.py) for documentation, check out the baseline config [here](https://github.com/chrisjmccormick/shared-subspaces/blob/main/subspace_encoder/configs/best_mla-o.json) to see all of the hyperparameters that are defined, and see the [Config](https://github.com/chrisjmccormick/shared-subspaces/blob/main/subspace_encoder/models/shared_space_config.py#L81) class for documentation of the model parameters.\n",
        "\n",
        "Below is an example for defining a new run which increases the output latent size to 96."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY-fQBEYeQ69",
        "outputId": "e567a46f-9dee-4777-e512-650d14bd118c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote new config to /content/shared-subspaces/subspace_encoder/configs/mla-o_baseline_o96.json\n"
          ]
        }
      ],
      "source": [
        "!python {base_path}/configs/create_new_config.py \\\n",
        "    mla-o_baseline_o96 \\\n",
        "    --base {base_path}/configs/mla-o_baseline.json \\\n",
        "    --shorthand \"rd.32 - 6.mla.64.32.96 - mlp.1024 - model.256.lyr.6 - ah.8.32\" \\\n",
        "    --notes \"Trying increasing the output subspace size from 64 to 96\" \\\n",
        "    --set model.o_latent_dim=96"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGuGV59klfBM",
        "outputId": "228bd8a5-78ef-45e2-81f1-7651141fb3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"shorthand\": \"rd.32 - 6.mla.64.32.96 - mlp.1024 - model.256.lyr.6 - ah.8.32\",\n",
            "  \"notes\": \"Trying increasing the output subspace size from 64 to 96\",\n",
            "  \"model\": {\n",
            "    \"hidden_size\": 256,\n",
            "    \"num_hidden_layers\": 6,\n",
            "    \"intermediate_size\": 1024,\n",
            "    \"hidden_dropout_prob\": 0.1,\n",
            "    \"attention_dropout_prob\": 0.1,\n",
            "    \"classifier_dropout\": null,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"rms_norm_eps\": 1e-06,\n",
            "    \"vocab_size\": 30522,\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"rope_scaling\": null,\n",
            "    \"max_position_embeddings\": 128,\n",
            "    \"num_dense_layers\": 0,\n",
            "    \"q_latent_dim\": 64,\n",
            "    \"kv_latent_dim\": 32,\n",
            "    \"num_attention_heads\": 8,\n",
            "    \"head_dim\": 32,\n",
            "    \"rope_dims\": 32,\n",
            "    \"attention_bias\": false,\n",
            "    \"output_subspace\": true,\n",
            "    \"o_latent_dim\": 96,\n",
            "    \"attention_backend\": \"sdpa\",\n",
            "    \"ffn_decompose\": false,\n",
            "    \"ffn_rank\": null,\n",
            "    \"vocab_subspace\": false,\n",
            "    \"vocab_rank\": 128\n",
            "  },\n",
            "  \"pre_train\": {\n",
            "    \"output_dir\": \"checkpoints/mla-o_baseline_o96\",\n",
            "    \"seed\": 42,\n",
            "    \"train_batch_size\": 256,\n",
            "    \"learning_rate\": 0.0005,\n",
            "    \"num_train_steps\": 50000,\n",
            "    \"eval_steps\": 2000,\n",
            "    \"weight_decay\": 0.01,\n",
            "    \"mlm_probability\": 0.15,\n",
            "    \"dataset_name\": \"wikitext\",\n",
            "    \"dataset_config\": \"wikitext-103-raw-v1\",\n",
            "    \"max_seq_length\": 128,\n",
            "    \"eval_batch_size\": 64,\n",
            "    \"fp16\": true\n",
            "  },\n",
            "  \"fine_tune\": {\n",
            "    \"task\": \"sst2\",\n",
            "    \"batch_size\": 16,\n",
            "    \"lr\": 2e-05,\n",
            "    \"epochs\": 3,\n",
            "    \"seed\": 42,\n",
            "    \"max_length\": 128\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "!cat {base_path}/configs/mla-o_baseline_o96.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3uaKyPGMiXW"
      },
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}